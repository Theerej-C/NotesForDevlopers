* Every time we run terraform it records the infrastructure info in a `Terraform state file`. 
* By default, when you run Terraform in the folder /foo/bar, Terraform creates the file /foo/bar/terraform.tfstate. This file contains a custom JSON format that records a mapping from the Terraform resources in your configuration files to the representation of those resources in the real world.
* The `terraform plan` output is the diff between the code and the real world infrastructure like the AWS.
* If it is for personal project then the single state file is no problem but if we work as a team there are problems arise when there is single terraform file.
* We need Shared Storage for state files, Locking state files for concurrency, Isolating State files.
* In this case we cannot rely on VCS for the storage and sharing so there is a mechanism in the Terraform named `backend`. Entire time we had done the local backend but there are `Remote backend` allow us to store the file in remote.
* AWS S3 is an excellent backend for the storage and sharing where we can define it easily in terraform itself.
* First we need to create a S3 bucket in the AWS. IT should be in diff folder than the last file.
* We set the `bucket` property uniquely as it is globally recognized. Then add the `prevent_destroy` to true.
* Next we need to set the versioning to be enabled so that we can revert back to the previous version. Like:
```
resource "aws_s3_bucket_versioning" "enabled" {
  bucket = aws_s3_bucket.state.id
  versioning_configuration {
    status = "Enabled"
  }
}
```
* Next use the `aws_server_side_encryption` which will turn server-side encryption on by default. Like:
```
resource "aws_s3_bucket_server_side_encryption_configuration" "default" {
    bucket = aws_s3_bucket.state.id
    rule {
      apply_server_side_encryption_by_default {
        sse_algorithm = "AES256"
      }
    }
}
```
* Then set the resource of S3 to be publicly blocked so that it is private:
```
resource "aws_s3_bucket_public_access_block" "public_access" {
  bucket                  = aws_s3_bucket.state.id
  block_public_acls       = true
  block_public_policy     = true
  ignore_public_acls      = true
  restrict_public_buckets = true
}
```
* Then we need to create a dynamo db for the consistency or the locking of operations. It is a key value store where we can store the key value pairs.
```
resource "aws_dynamodb_table" "terraform_lock" {
  name         = "terraform-up-ex"
  billing_mode = "PAY_PER_REQUEST"
  hash_key     = "LockID"
  attribute {
    name = "LockID"
    type = "S"
  }
}
```
* So as this is created we need to set the backend to store the state file in this S3 bucket so that it will be available to the team.
* It has to be done in the `terraform` block with the `backend` and the configuration.
```
terraform {
  backend "s3" {
    bucket = "terraform-state-ex"
    key = "global/s3/terraform.tfstate"
    region = "us-east-1"

    dynamodb_table = "terraform-up-ex"
    encrypt = true
  }
}
```
* Here the key is the place in the S3 where we need to store the state file. To work this config has to be initialised again using the `init`.
## Limitations:
* There are some major limitations in the terraform backend like chicken egg state.
* The first one as mentioned is the chicken egg state where we need to create a S3 bucket to do the storage of the state where this has to be configured by the terraform itself.
* So there is two steps involved in it so we need to do the reverse two steps to undo it.
* Then the second most annoying one is that the variables are not allowed in the backend so it is difficult to use the code as there are much more boiler plate.
## State file Isolation:
* 